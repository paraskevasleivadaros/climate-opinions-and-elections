{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicate PIDs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are duplicate PIDs in the dataset.\n",
      "Duplicate PIDs have been saved to '../data/duplicate_pids.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('../data/w1w2w3w4w5_indices_weights_jul12_2022.csv', low_memory=False)\n",
    "\n",
    "# Check for duplicate PIDs\n",
    "duplicate_pids = df[df.duplicated('PID', keep=False)]\n",
    "\n",
    "if not duplicate_pids.empty:\n",
    "    print(\"There are duplicate PIDs in the dataset.\")\n",
    "    duplicate_pids.to_csv('../data/duplicate_pids.csv', index=False)\n",
    "    print(\"Duplicate PIDs have been saved to '../data/duplicate_pids.csv'.\")\n",
    "else:\n",
    "    print(\"All PIDs are unique.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Filter Wave Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to filtered_duplicate_pids.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"../data/duplicate_pids.csv\", low_memory=False)\n",
    "\n",
    "# Identify valid sequences (rolling window approach)\n",
    "df['valid_seq'] = df['WAVE'].rolling(3).apply(lambda x: list(x) == [2,3,4], raw=True)\n",
    "\n",
    "# Flag rows belonging to valid sequences\n",
    "df['keep'] = df['valid_seq'].shift(-2).fillna(0).astype(bool) | df['valid_seq'].shift(-1).fillna(0).astype(bool) | df['valid_seq'].fillna(0).astype(bool)\n",
    "\n",
    "# Filter the rows\n",
    "filtered_df = df[df['keep']].drop(columns=['valid_seq', 'keep'])\n",
    "\n",
    "# Save to a new CSV file\n",
    "filtered_df.to_csv(\"../data/filtered_duplicate_pids.csv\", index=False)\n",
    "\n",
    "print(\"Filtered data saved to filtered_duplicate_pids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Likert Scale Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been standardized and saved to 'data/likert_scale.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file from the data directory\n",
    "df = pd.read_csv('../data/filtered_duplicate_pids.csv', low_memory=False)\n",
    "\n",
    "# Some variables use a five-point Likert scale (1 to 5), while others use a four-point scale (1 to 4). \n",
    "# Since some scales are missing a neutral category (like 3), we need to standardize them.\n",
    "# I chose to rescale the 1-4 scale to match the 1-5 scaleby applying a linear transformation:\n",
    "# 1 (Not at all)         → 1\n",
    "# 2 (Only a little)      → 2.33\n",
    "# 3 (A moderate amount)  → 3.67\n",
    "# 4 (A great deal)       → 5\n",
    "mapping = {1: 1, 2: 2.33, 3: 3.67, 4: 5}\n",
    "\n",
    "# List of variables to be mapped\n",
    "variables_to_map = ['cc4_world', 'cc4_wealthUS', 'cc4_poorUS', 'cc4_comm', 'cc4_famheal', 'cc4_famecon']\n",
    "\n",
    "# Apply the mapping to the specified variables\n",
    "for var in variables_to_map:\n",
    "    df[var] = df[var].map(mapping)\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "df.to_csv('../data/likert_scale.csv', index=False)\n",
    "\n",
    "print(\"Data has been standardized and saved to 'data/likert_scale.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from skimpy import skim  # Equivalent to skimr::skim() in R\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/likert_scale.csv', low_memory=False)\n",
    "\n",
    "# Define ccSolve columns in decreasing order of WTP\n",
    "ccSolve_columns = [\"ccSolve100\", \"ccSolve50\", \"ccSolve10\", \"ccSolve1\", \"ccSolve0\"]\n",
    "cc_policy_columns = [\"cc_pol_tax\", \"cc_pol_car\"]\n",
    "\n",
    "# Step 1: Fill downward (Lower WTP should be at least as approved as higher WTP)\n",
    "for i in range(len(ccSolve_columns) - 1):  \n",
    "    higher_col = ccSolve_columns[i]\n",
    "    lower_col = ccSolve_columns[i + 1]\n",
    "\n",
    "    # If lower response is missing but higher response exists, fill with gradual increase\n",
    "    df[lower_col] = df[lower_col].fillna(df[higher_col] + 0.75)\n",
    "\n",
    "# Step 2: Fill upward (Higher WTP should be less approved than lower WTP)\n",
    "for i in range(len(ccSolve_columns) - 1, 0, -1):  \n",
    "    lower_col = ccSolve_columns[i]\n",
    "    higher_col = ccSolve_columns[i - 1]\n",
    "\n",
    "    # If higher response is missing but lower response exists, fill with gradual decrease\n",
    "    df[higher_col] = df[higher_col].fillna(df[lower_col] - 1.5)\n",
    "\n",
    "# Step 3: Handle respondents with all missing `ccSolve` responses\n",
    "# If all `ccSolve` values are missing for a respondent, fill with a neutral value (3)\n",
    "df[ccSolve_columns] = df[ccSolve_columns].apply(lambda row: row.fillna(3) if row.isna().all() else row, axis=1)\n",
    "\n",
    "# Step 4: Ensure values stay within 1-5\n",
    "df[ccSolve_columns] = df[ccSolve_columns].clip(1, 5)\n",
    "\n",
    "# Step 5: Fill missing values for cc_pol_tax and cc_pol_car with 3 (Neutral)\n",
    "for cc_policy in cc_policy_columns:\n",
    "    df[cc_policy] = df[cc_policy].fillna(3)\n",
    "\n",
    "# Save the filled dataset\n",
    "df.to_csv(\"../data/cleaned_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
